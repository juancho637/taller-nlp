"""
ü§ñ MODELO 1 - TRANSFORMER SIMPLE
================================

Modelo 1: Una capa Transformer para generaci√≥n estable y coherente.

Caracter√≠sticas:
- ‚úÖ Una sola capa de atenci√≥n
- ‚úÖ Entrenamiento estable y r√°pido
- ‚úÖ Menos propenso al overfitting
- ‚úÖ Ideal para principiantes
"""

import tensorflow as tf
from tensorflow import keras
from keras import layers
import os

# ============================================================================
# CONFIGURACI√ìN MODELO 1
# ============================================================================

class Model1Config:
    """Configuraci√≥n espec√≠fica para el Modelo 1"""
    
    # Identificaci√≥n
    MODEL_NAME = "1"
    MODEL_DESCRIPTION = "Transformer Simple (Una capa de atenci√≥n)"
    
    # Arquitectura espec√≠fica
    NUM_TRANSFORMER_LAYERS = 1      # Solo UNA capa transformer
    DROPOUT_RATE = 0.1              # Dropout bajo para estabilidad
    
    # Par√°metros del modelo (heredados de configuraci√≥n base)
    VOCAB_SIZE = 5000
    SEQUENCE_LENGTH = 60
    EMBED_DIM = 128
    NUM_HEADS = 4
    LATENT_DIM = 256
    
    # Entrenamiento
    EPOCHS = 15                     # Menos √©pocas, convergencia estable
    LEARNING_RATE = 0.001           # Learning rate est√°ndar
    BATCH_SIZE = 32
    
    # Callbacks
    EARLY_STOPPING_PATIENCE = 5     # M√°s paciencia para convergencia estable
    REDUCE_LR_PATIENCE = 3
    REDUCE_LR_FACTOR = 0.5
    
    # Objetivos
    TARGET_LOSS = 4.5               # P√©rdida objetivo realista
    
    @classmethod
    def print_info(cls):
        """Imprime informaci√≥n del modelo"""
        print("ü§ñ MODELO 1 - TRANSFORMER SIMPLE")
        print("=" * 45)
        print(f"üìù Descripci√≥n: {cls.MODEL_DESCRIPTION}")
        print(f"üèóÔ∏è Capas Transformer: {cls.NUM_TRANSFORMER_LAYERS}")
        print(f"üéì √âpocas: {cls.EPOCHS}")
        print(f"üìà Learning Rate: {cls.LEARNING_RATE}")
        print(f"üéØ P√©rdida objetivo: {cls.TARGET_LOSS}")
        print(f"üíß Dropout: {cls.DROPOUT_RATE}")
        print()
        print("‚ú® Fortalezas:")
        print("   - Entrenamiento estable")
        print("   - Convergencia r√°pida")
        print("   - Menos overfitting")
        print("   - Ideal para principiantes")
        print("   - Resultados predecibles")
        print()
        print("üéØ Mejor para:")
        print("   - Usuarios nuevos en NLP")
        print("   - Aplicaciones que requieren estabilidad")
        print("   - Texto coherente y conservador")

# ============================================================================
# CAPA PERSONALIZADA (COMPARTIDA)
# ============================================================================

class PositionalEmbedding(layers.Layer):
    """Capa personalizada para embeddings posicionales"""
    
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        
        self.token_embeddings = layers.Embedding(vocab_size, embed_dim)
        self.position_embeddings = layers.Embedding(sequence_length, embed_dim)
    
    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        
        token_emb = self.token_embeddings(inputs)
        pos_emb = self.position_embeddings(positions)
        
        return token_emb + pos_emb
    
    def get_config(self):
        config = super().get_config()
        config.update({
            "sequence_length": self.sequence_length,
            "vocab_size": self.vocab_size,
            "embed_dim": self.embed_dim,
        })
        return config
    
    @classmethod
    def from_config(cls, config):
        return cls(**config)

# ============================================================================
# ARQUITECTURA DEL MODELO 1
# ============================================================================

def create_model_1():
    """
    Crea el Modelo 1: Transformer Simple con una sola capa de atenci√≥n.
    
    Arquitectura:
    - Embeddings posicionales
    - 1 capa de Multi-Head Attention
    - 1 red Feed-Forward
    - Layer Normalization
    - Capa de salida
    
    Returns:
        Modelo compilado listo para entrenar
    """
    print("üèóÔ∏è Creando Modelo 1 - Transformer Simple...")
    
    config = Model1Config()
    
    # Entrada
    inputs = keras.Input(shape=(None,), dtype="int32")
    
    # Embeddings de tokens y posiciones
    x = PositionalEmbedding(
        sequence_length=config.SEQUENCE_LENGTH,
        vocab_size=config.VOCAB_SIZE,
        embed_dim=config.EMBED_DIM
    )(inputs)
    
    # √öNICA capa de atenci√≥n m√∫ltiple
    attention = layers.MultiHeadAttention(
        num_heads=config.NUM_HEADS, 
        key_dim=config.EMBED_DIM,
        dropout=config.DROPOUT_RATE
    )(x, x)
    
    # Conexi√≥n residual + Layer Norm
    x = layers.LayerNormalization()(x + attention)
    
    # Red Feed-Forward
    ffn = keras.Sequential([
        layers.Dense(config.LATENT_DIM, activation="relu"),
        layers.Dropout(config.DROPOUT_RATE),
        layers.Dense(config.EMBED_DIM)
    ])(x)
    
    # Conexi√≥n residual + Layer Norm
    x = layers.LayerNormalization()(x + ffn)
    
    # Capa de salida - predicci√≥n de siguiente palabra
    outputs = layers.Dense(config.VOCAB_SIZE, activation="softmax")(x)
    
    # Crear y compilar modelo
    model = keras.Model(inputs, outputs, name="transformer_model_1")
    
    model.compile(
        optimizer=keras.optimizers.Adam(config.LEARNING_RATE),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )
    
    total_params = model.count_params()
    print(f"‚úÖ Modelo 1 creado exitosamente")
    print(f"üìä Par√°metros totales: {total_params:,}")
    print(f"üéØ Configurado para {config.EPOCHS} √©pocas con LR={config.LEARNING_RATE}")
    
    return model

# ============================================================================
# ENTRENAMIENTO DEL MODELO 1
# ============================================================================

def train_model_1(train_dataset, text_vectorization, verbose=True):
    """
    Entrena el Modelo 1 con configuraci√≥n optimizada.
    
    Args:
        train_dataset: Dataset de entrenamiento preparado
        text_vectorization: Vectorizador de texto
        verbose: Si mostrar informaci√≥n detallada
        
    Returns:
        tuple: (modelo_entrenado, historial_entrenamiento)
    """
    config = Model1Config()
    
    if verbose:
        print(f"\nüèãÔ∏è ENTRENANDO MODELO 1")
        print("=" * 50)
        config.print_info()
    
    # Crear directorio si no existe
    os.makedirs("saved_models", exist_ok=True)
    
    # Crear modelo
    model = create_model_1()
    
    if verbose:
        print("\nüìã Resumen del modelo:")
        model.summary()
    
    # Configurar callbacks para entrenamiento estable
    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor='loss',
            patience=config.EARLY_STOPPING_PATIENCE,
            restore_best_weights=True,
            verbose=1 if verbose else 0
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='loss',
            factor=config.REDUCE_LR_FACTOR,
            patience=config.REDUCE_LR_PATIENCE,
            verbose=1 if verbose else 0,
            min_lr=0.0001
        ),
        keras.callbacks.ModelCheckpoint(
            filepath="saved_models/movie_model_1_best.keras",
            monitor='loss',
            save_best_only=True,
            verbose=1 if verbose else 0
        )
    ]
    
    # Entrenar modelo
    if verbose:
        print(f"\nüöÄ Iniciando entrenamiento...")
        print(f"‚è±Ô∏è √âpocas m√°ximas: {config.EPOCHS}")
        print(f"üéØ P√©rdida objetivo: {config.TARGET_LOSS}")
    
    history = model.fit(
        train_dataset,
        epochs=config.EPOCHS,
        callbacks=callbacks,
        verbose=1 if verbose else 2
    )
    
    # Guardar modelo final
    model.save("saved_models/movie_model_1.keras")
    
    # Mostrar resultados
    final_loss = min(history.history['loss'])
    final_accuracy = max(history.history['accuracy'])
    
    if verbose:
        print(f"\n‚úÖ ENTRENAMIENTO MODELO 1 COMPLETADO")
        print("=" * 50)
        print(f"üìâ Mejor p√©rdida: {final_loss:.4f}")
        print(f"üìà Mejor precisi√≥n: {final_accuracy:.4f}")
        print(f"üéØ Objetivo alcanzado: {'‚úÖ S√ç' if final_loss <= config.TARGET_LOSS else '‚ùå NO'}")
        print(f"üíæ Modelo guardado en: saved_models/movie_model_1.keras")
        print(f"üìä √âpocas ejecutadas: {len(history.history['loss'])}")
    
    return model, history

# ============================================================================
# FUNCI√ìN DE UTILIDAD
# ============================================================================

def get_model_1_info():
    """
    Obtiene informaci√≥n completa del Modelo 1.
    
    Returns:
        dict: Informaci√≥n del modelo
    """
    config = Model1Config()
    
    return {
        'name': config.MODEL_NAME,
        'description': config.MODEL_DESCRIPTION,
        'transformer_layers': config.NUM_TRANSFORMER_LAYERS,
        'epochs': config.EPOCHS,
        'learning_rate': config.LEARNING_RATE,
        'target_loss': config.TARGET_LOSS,
        'dropout_rate': config.DROPOUT_RATE,
        'strengths': [
            "Entrenamiento estable",
            "Convergencia r√°pida",
            "Menos overfitting",
            "Ideal para principiantes",
            "Resultados predecibles"
        ],
        'best_for': [
            "Usuarios nuevos en NLP",
            "Aplicaciones que requieren estabilidad",
            "Texto coherente y conservador"
        ],
        'recommended_temperature': 0.7,
        'recommended_length': 40
    }

# Registrar la capa personalizada
keras.utils.get_custom_objects()['PositionalEmbedding'] = PositionalEmbedding