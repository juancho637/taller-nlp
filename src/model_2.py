"""
ü§ñ MODELO 2 - TRANSFORMER DOBLE
===============================

Modelo 2: Dos capas Transformer para generaci√≥n m√°s creativa y expresiva.

Caracter√≠sticas:
- ‚úÖ Dos capas de atenci√≥n
- ‚úÖ Mayor capacidad expresiva
- ‚úÖ M√°s creativo en la generaci√≥n
- ‚úÖ Ideal para usuarios avanzados
"""

import tensorflow as tf
from tensorflow import keras
from keras import layers
import os

# ============================================================================
# CONFIGURACI√ìN MODELO 2
# ============================================================================

class Model2Config:
    """Configuraci√≥n espec√≠fica para el Modelo 2"""
    
    # Identificaci√≥n
    MODEL_NAME = "2"
    MODEL_DESCRIPTION = "Transformer Doble (Dos capas de atenci√≥n)"
    
    # Arquitectura espec√≠fica
    NUM_TRANSFORMER_LAYERS = 2      # DOS capas transformer
    DROPOUT_RATE = 0.2              # Dropout m√°s alto para regularizaci√≥n
    
    # Par√°metros del modelo (heredados de configuraci√≥n base)
    VOCAB_SIZE = 5000
    SEQUENCE_LENGTH = 60
    EMBED_DIM = 128
    NUM_HEADS = 4
    LATENT_DIM = 256
    
    # Entrenamiento (m√°s cuidadoso para evitar overfitting)
    EPOCHS = 20                     # M√°s √©pocas pero con early stopping agresivo
    LEARNING_RATE = 0.0005          # Learning rate m√°s bajo para estabilidad
    BATCH_SIZE = 32
    
    # Callbacks m√°s agresivos
    EARLY_STOPPING_PATIENCE = 3     # Menos paciencia para evitar overfitting
    REDUCE_LR_PATIENCE = 2          # Reducir LR m√°s r√°pido
    REDUCE_LR_FACTOR = 0.7          # Reducci√≥n m√°s suave
    
    # Regularizaci√≥n adicional
    WEIGHT_DECAY = 0.0001           # Regularizaci√≥n L2
    GRADIENT_CLIP_NORM = 1.0        # Gradient clipping
    
    # Objetivos
    TARGET_LOSS = 4.0               # P√©rdida objetivo m√°s ambiciosa
    
    @classmethod
    def print_info(cls):
        """Imprime informaci√≥n del modelo"""
        print("ü§ñ MODELO 2 - TRANSFORMER DOBLE")
        print("=" * 45)
        print(f"üìù Descripci√≥n: {cls.MODEL_DESCRIPTION}")
        print(f"üèóÔ∏è Capas Transformer: {cls.NUM_TRANSFORMER_LAYERS}")
        print(f"üéì √âpocas: {cls.EPOCHS}")
        print(f"üìà Learning Rate: {cls.LEARNING_RATE}")
        print(f"üéØ P√©rdida objetivo: {cls.TARGET_LOSS}")
        print(f"üíß Dropout: {cls.DROPOUT_RATE}")
        print(f"‚öñÔ∏è Weight Decay: {cls.WEIGHT_DECAY}")
        print()
        print("‚ú® Fortalezas:")
        print("   - Mayor capacidad expresiva")
        print("   - Generaci√≥n m√°s creativa")
        print("   - Mejor comprensi√≥n del contexto")
        print("   - M√°s variedad en el texto")
        print("   - Ideal para contenido original")
        print()
        print("‚ö†Ô∏è Desaf√≠os:")
        print("   - M√°s propenso al overfitting")
        print("   - Requiere ajuste cuidadoso")
        print("   - Entrenamiento m√°s lento")
        print()
        print("üéØ Mejor para:")
        print("   - Usuarios avanzados en NLP")
        print("   - Aplicaciones creativas")
        print("   - Texto expresivo y variado")

# ============================================================================
# CAPA PERSONALIZADA (COMPARTIDA)
# ============================================================================

class PositionalEmbedding(layers.Layer):
    """Capa personalizada para embeddings posicionales"""
    
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        
        self.token_embeddings = layers.Embedding(vocab_size, embed_dim)
        self.position_embeddings = layers.Embedding(sequence_length, embed_dim)
    
    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        
        token_emb = self.token_embeddings(inputs)
        pos_emb = self.position_embeddings(positions)
        
        return token_emb + pos_emb
    
    def get_config(self):
        config = super().get_config()
        config.update({
            "sequence_length": self.sequence_length,
            "vocab_size": self.vocab_size,
            "embed_dim": self.embed_dim,
        })
        return config
    
    @classmethod
    def from_config(cls, config):
        return cls(**config)

# ============================================================================
# ARQUITECTURA DEL MODELO 2
# ============================================================================

def create_model_2():
    """
    Crea el Modelo 2: Transformer Doble con dos capas de atenci√≥n.
    
    Arquitectura:
    - Embeddings posicionales
    - 1ra capa de Multi-Head Attention + FFN
    - 2da capa de Multi-Head Attention + FFN
    - Layer Normalization en cada bloque
    - Capa de salida
    
    Returns:
        Modelo compilado listo para entrenar
    """
    print("üèóÔ∏è Creando Modelo 2 - Transformer Doble...")
    
    config = Model2Config()
    
    # Entrada
    inputs = keras.Input(shape=(None,), dtype="int32")
    
    # Embeddings de tokens y posiciones
    x = PositionalEmbedding(
        sequence_length=config.SEQUENCE_LENGTH,
        vocab_size=config.VOCAB_SIZE,
        embed_dim=config.EMBED_DIM
    )(inputs)
    
    # PRIMERA capa Transformer
    # Multi-Head Attention 1
    attention1 = layers.MultiHeadAttention(
        num_heads=config.NUM_HEADS,
        key_dim=config.EMBED_DIM,
        dropout=config.DROPOUT_RATE
    )(x, x)
    
    # Conexi√≥n residual + Layer Norm
    x = layers.LayerNormalization()(x + attention1)
    
    # Feed-Forward Network 1
    ffn1 = keras.Sequential([
        layers.Dense(config.LATENT_DIM, activation="relu"),
        layers.Dropout(config.DROPOUT_RATE),
        layers.Dense(config.EMBED_DIM)
    ])(x)
    
    # Conexi√≥n residual + Layer Norm
    x = layers.LayerNormalization()(x + ffn1)
    
    # SEGUNDA capa Transformer (lo que lo hace m√°s poderoso)
    # Multi-Head Attention 2
    attention2 = layers.MultiHeadAttention(
        num_heads=config.NUM_HEADS,
        key_dim=config.EMBED_DIM,
        dropout=config.DROPOUT_RATE
    )(x, x)
    
    # Conexi√≥n residual + Layer Norm
    x = layers.LayerNormalization()(x + attention2)
    
    # Feed-Forward Network 2
    ffn2 = keras.Sequential([
        layers.Dense(config.LATENT_DIM, activation="relu"),
        layers.Dropout(config.DROPOUT_RATE),
        layers.Dense(config.EMBED_DIM)
    ])(x)
    
    # Conexi√≥n residual + Layer Norm
    x = layers.LayerNormalization()(x + ffn2)
    
    # Capa de salida - predicci√≥n de siguiente palabra
    outputs = layers.Dense(config.VOCAB_SIZE, activation="softmax")(x)
    
    # Crear y compilar modelo
    model = keras.Model(inputs, outputs, name="transformer_model_2")
    
    # Compilar con regularizaci√≥n adicional
    optimizer = keras.optimizers.Adam(
        learning_rate=config.LEARNING_RATE,
        weight_decay=config.WEIGHT_DECAY  # Regularizaci√≥n L2
    )
    
    model.compile(
        optimizer=optimizer,
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )
    
    total_params = model.count_params()
    print(f"‚úÖ Modelo 2 creado exitosamente")
    print(f"üìä Par√°metros totales: {total_params:,}")
    print(f"üéØ Configurado para {config.EPOCHS} √©pocas con LR={config.LEARNING_RATE}")
    print(f"üõ°Ô∏è Regularizaci√≥n: Dropout={config.DROPOUT_RATE}, Weight Decay={config.WEIGHT_DECAY}")
    
    return model

# ============================================================================
# ENTRENAMIENTO DEL MODELO 2
# ============================================================================

def train_model_2(train_dataset, text_vectorization, verbose=True):
    """
    Entrena el Modelo 2 con configuraci√≥n cuidadosa para evitar overfitting.
    
    Args:
        train_dataset: Dataset de entrenamiento preparado
        text_vectorization: Vectorizador de texto
        verbose: Si mostrar informaci√≥n detallada
        
    Returns:
        tuple: (modelo_entrenado, historial_entrenamiento)
    """
    config = Model2Config()
    
    if verbose:
        print(f"\nüèãÔ∏è ENTRENANDO MODELO 2")
        print("=" * 50)
        config.print_info()
    
    # Crear directorio si no existe
    os.makedirs("saved_models", exist_ok=True)
    
    # Crear modelo
    model = create_model_2()
    
    if verbose:
        print("\nüìã Resumen del modelo:")
        model.summary()
    
    # Configurar callbacks M√ÅS RESTRICTIVOS para evitar overfitting
    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor='loss',
            patience=config.EARLY_STOPPING_PATIENCE,  # MENOS paciencia
            restore_best_weights=True,
            verbose=1 if verbose else 0
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='loss',
            factor=config.REDUCE_LR_FACTOR,  # Reducci√≥n m√°s suave
            patience=config.REDUCE_LR_PATIENCE,  # M√°s r√°pido
            verbose=1 if verbose else 0,
            min_lr=0.00001
        ),
        keras.callbacks.ModelCheckpoint(
            filepath="saved_models/movie_model_2_best.keras",
            monitor='loss',
            save_best_only=True,
            verbose=1 if verbose else 0
        )
    ]
    
    # Agregar gradient clipping si es necesario
    if hasattr(model.optimizer, 'clipnorm'):
        model.optimizer.clipnorm = config.GRADIENT_CLIP_NORM
    
    # Entrenar modelo
    if verbose:
        print(f"\nüöÄ Iniciando entrenamiento...")
        print(f"‚è±Ô∏è √âpocas m√°ximas: {config.EPOCHS}")
        print(f"üéØ P√©rdida objetivo: {config.TARGET_LOSS}")
        print(f"‚ö†Ô∏è Early stopping agresivo: {config.EARLY_STOPPING_PATIENCE} √©pocas")
    
    history = model.fit(
        train_dataset,
        epochs=config.EPOCHS,
        callbacks=callbacks,
        verbose=1 if verbose else 2
    )
    
    # Guardar modelo final
    model.save("saved_models/movie_model_2.keras")
    
    # Mostrar resultados
    final_loss = min(history.history['loss'])
    final_accuracy = max(history.history['accuracy'])
    
    if verbose:
        print(f"\n‚úÖ ENTRENAMIENTO MODELO 2 COMPLETADO")
        print("=" * 50)
        print(f"üìâ Mejor p√©rdida: {final_loss:.4f}")
        print(f"üìà Mejor precisi√≥n: {final_accuracy:.4f}")
        print(f"üéØ Objetivo alcanzado: {'‚úÖ S√ç' if final_loss <= config.TARGET_LOSS else '‚ùå NO'}")
        print(f"üíæ Modelo guardado en: saved_models/movie_model_2.keras")
        print(f"üìä √âpocas ejecutadas: {len(history.history['loss'])}")
        
        if final_loss > config.TARGET_LOSS:
            print(f"\nüí° Sugerencias para mejorar:")
            print(f"   - Reducir learning rate a 0.0003")
            print(f"   - Aumentar regularizaci√≥n (dropout a 0.3)")
            print(f"   - Usar m√°s datos de entrenamiento")
    
    return model, history

# ============================================================================
# FUNCI√ìN DE UTILIDAD
# ============================================================================

def get_model_2_info():
    """
    Obtiene informaci√≥n completa del Modelo 2.
    
    Returns:
        dict: Informaci√≥n del modelo
    """
    config = Model2Config()
    
    return {
        'name': config.MODEL_NAME,
        'description': config.MODEL_DESCRIPTION,
        'transformer_layers': config.NUM_TRANSFORMER_LAYERS,
        'epochs': config.EPOCHS,
        'learning_rate': config.LEARNING_RATE,
        'target_loss': config.TARGET_LOSS,
        'dropout_rate': config.DROPOUT_RATE,
        'weight_decay': config.WEIGHT_DECAY,
        'strengths': [
            "Mayor capacidad expresiva",
            "Generaci√≥n m√°s creativa",
            "Mejor comprensi√≥n del contexto",
            "M√°s variedad en el texto",
            "Ideal para contenido original"
        ],
        'challenges': [
            "M√°s propenso al overfitting",
            "Requiere ajuste cuidadoso",
            "Entrenamiento m√°s lento"
        ],
        'best_for': [
            "Usuarios avanzados en NLP",
            "Aplicaciones creativas",
            "Texto expresivo y variado"
        ],
        'recommended_temperature': 0.9,
        'recommended_length': 50
    }

def compare_with_model_1():
    """
    Compara el Modelo 2 con el Modelo 1.
    
    Returns:
        dict: Comparaci√≥n detallada
    """
    return {
        'complexity': {
            'model_1': '1 capa transformer',
            'model_2': '2 capas transformer',
            'difference': 'Modelo 2 es ~60% m√°s complejo'
        },
        'training': {
            'model_1': 'M√°s estable, convergencia suave',
            'model_2': 'M√°s variable, requiere monitoreo',
            'recommendation': 'Usar Modelo 1 primero, luego experimentar con Modelo 2'
        },
        'output_quality': {
            'model_1': 'Coherente y predecible',
            'model_2': 'M√°s creativo pero menos predecible',
            'use_case': 'Modelo 1 para uso general, Modelo 2 para creatividad'
        }
    }

# Registrar la capa personalizada
keras.utils.get_custom_objects()['PositionalEmbedding'] = PositionalEmbedding