"""
üèãÔ∏è ENTRENAMIENTO DE MODELOS TRANSFORMER
=======================================

Script para entrenar los modelos por separado.

Uso:
    python train.py --modelo 1    # Solo Modelo 1
    python train.py --modelo 2    # Solo Modelo 2
    python train.py --modelo all  # Ambos modelos
"""

import tensorflow as tf
from tensorflow import keras
from keras import layers
import numpy as np
import argparse
import pickle
import os

# ============================================================================
# CONFIGURACI√ìN
# ============================================================================

class Config:
    """Configuraci√≥n para el entrenamiento"""
    # Par√°metros del modelo
    VOCAB_SIZE = 5000           # Vocabulario de palabras
    SEQUENCE_LENGTH = 60        # Longitud m√°xima de secuencia
    EMBED_DIM = 128            # Dimensi√≥n de embeddings
    NUM_HEADS = 4              # Cabezas de atenci√≥n
    LATENT_DIM = 256           # Dimensi√≥n de la capa densa
    
    # Entrenamiento
    BATCH_SIZE = 32
    SAMPLE_SIZE = 2000         # Muestras del dataset
    
    # Configuraci√≥n espec√≠fica por modelo
    MODEL1_EPOCHS = 15         # Modelo 1: Menos √©pocas, m√°s estable
    MODEL1_LR = 0.001         # Learning rate normal
    
    MODEL2_EPOCHS = 10         # Modelo 2: Pocas √©pocas para evitar overfitting
    MODEL2_LR = 0.0005        # Learning rate m√°s bajo

# ============================================================================
# CAPA PERSONALIZADA PARA EMBEDDINGS POSICIONALES
# ============================================================================

class PositionalEmbedding(layers.Layer):
    """Capa personalizada para embeddings posicionales"""
    
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        
        self.token_embeddings = layers.Embedding(vocab_size, embed_dim)
        self.position_embeddings = layers.Embedding(sequence_length, embed_dim)
    
    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        
        token_emb = self.token_embeddings(inputs)
        pos_emb = self.position_embeddings(positions)
        
        return token_emb + pos_emb
    
    def get_config(self):
        config = super().get_config()
        config.update({
            "sequence_length": self.sequence_length,
            "vocab_size": self.vocab_size,
            "embed_dim": self.embed_dim,
        })
        return config
    
    @classmethod
    def from_config(cls, config):
        return cls(**config)

# Registrar la capa personalizada de forma manual
keras.utils.get_custom_objects()['PositionalEmbedding'] = PositionalEmbedding

# ============================================================================
# MODELO 1: TRANSFORMER SIMPLE (M√ÅS ESTABLE)
# ============================================================================

def create_model_1():
    """
    Modelo 1: Una sola capa Transformer
    - M√°s estable y f√°cil de entrenar
    - Menos par√°metros, menos overfitting
    - Recomendado para empezar
    """
    print("üèóÔ∏è Creando Modelo 1 (Simple)...")
    
    inputs = keras.Input(shape=(None,), dtype="int32")
    
    # Embeddings de tokens y posiciones usando la capa personalizada
    x = PositionalEmbedding(
        sequence_length=Config.SEQUENCE_LENGTH,
        vocab_size=Config.VOCAB_SIZE,
        embed_dim=Config.EMBED_DIM
    )(inputs)
    
    # UNA SOLA capa de atenci√≥n
    attention = layers.MultiHeadAttention(
        num_heads=Config.NUM_HEADS, 
        key_dim=Config.EMBED_DIM
    )(x, x)
    x = layers.LayerNormalization()(x + attention)
    
    # Red neuronal feed-forward
    ffn = keras.Sequential([
        layers.Dense(Config.LATENT_DIM, activation="relu"),
        layers.Dense(Config.EMBED_DIM)
    ])(x)
    x = layers.LayerNormalization()(x + ffn)
    
    # Capa de salida
    outputs = layers.Dense(Config.VOCAB_SIZE, activation="softmax")(x)
    
    model = keras.Model(inputs, outputs)
    model.compile(
        optimizer=keras.optimizers.Adam(Config.MODEL1_LR),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )
    
    print(f"‚úÖ Modelo 1 creado: {model.count_params():,} par√°metros")
    return model

# ============================================================================
# MODELO 2: TRANSFORMER DOBLE (M√ÅS COMPLEJO)
# ============================================================================

def create_model_2():
    """
    Modelo 2: Dos capas Transformer
    - M√°s expresivo pero puede hacer overfitting
    - M√°s par√°metros, m√°s capacidad
    - Requiere entrenamiento cuidadoso
    """
    print("üèóÔ∏è Creando Modelo 2 (Doble)...")
    
    inputs = keras.Input(shape=(None,), dtype="int32")
    
    # Embeddings usando la capa personalizada
    x = PositionalEmbedding(
        sequence_length=Config.SEQUENCE_LENGTH,
        vocab_size=Config.VOCAB_SIZE,
        embed_dim=Config.EMBED_DIM
    )(inputs)
    
    # PRIMERA capa de atenci√≥n
    attention1 = layers.MultiHeadAttention(
        num_heads=Config.NUM_HEADS, 
        key_dim=Config.EMBED_DIM
    )(x, x)
    x = layers.LayerNormalization()(x + attention1)
    
    ffn1 = keras.Sequential([
        layers.Dense(Config.LATENT_DIM, activation="relu"),
        layers.Dense(Config.EMBED_DIM)
    ])(x)
    x = layers.LayerNormalization()(x + ffn1)
    
    # SEGUNDA capa de atenci√≥n (lo que lo hace m√°s poderoso)
    attention2 = layers.MultiHeadAttention(
        num_heads=Config.NUM_HEADS, 
        key_dim=Config.EMBED_DIM
    )(x, x)
    x = layers.LayerNormalization()(x + attention2)
    
    ffn2 = keras.Sequential([
        layers.Dense(Config.LATENT_DIM, activation="relu"),
        layers.Dense(Config.EMBED_DIM)
    ])(x)
    x = layers.LayerNormalization()(x + ffn2)
    
    # Capa de salida
    outputs = layers.Dense(Config.VOCAB_SIZE, activation="softmax")(x)
    
    model = keras.Model(inputs, outputs)
    model.compile(
        optimizer=keras.optimizers.Adam(Config.MODEL2_LR),  # LR m√°s bajo
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )
    
    print(f"‚úÖ Modelo 2 creado: {model.count_params():,} par√°metros")
    return model

# ============================================================================
# PREPARACI√ìN DE DATOS (COMPARTIDA)
# ============================================================================

def prepare_data():
    """
    Prepara los datos de IMDb para entrenamiento.
    Se usa para ambos modelos.
    """
    print("üìä Preparando datos de IMDb...")
    
    # Cargar dataset
    (x_train, _), _ = keras.datasets.imdb.load_data()
    word_index = keras.datasets.imdb.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    
    # Convertir a texto
    def decode_review(text):
        return ' '.join([reverse_word_index.get(i - 3, '?') for i in text])
    
    text_data = []
    for i in range(min(Config.SAMPLE_SIZE, len(x_train))):
        review_text = decode_review(x_train[i])
        text_data.append(review_text)
    
    # Crear vectorizador
    dataset = tf.data.Dataset.from_tensor_slices(text_data).batch(Config.BATCH_SIZE)
    
    text_vectorization = layers.TextVectorization(
        max_tokens=Config.VOCAB_SIZE,
        output_mode="int",
        output_sequence_length=Config.SEQUENCE_LENGTH,
    )
    text_vectorization.adapt(dataset)
    
    # Preparar secuencias de entrenamiento
    def prepare_sequences(text_batch):
        vectorized = text_vectorization(text_batch)
        x = vectorized[:, :-1]
        y = vectorized[:, 1:]
        return x, y
    
    train_dataset = dataset.map(prepare_sequences)
    
    print("‚úÖ Datos preparados")
    return train_dataset, text_vectorization

def save_vectorizer(text_vectorization):
    """Guardar el vectorizador para usar en la app"""
    # Crear carpeta si no existe
    os.makedirs("saved_models", exist_ok=True)
    
    vectorizer_data = {
        'config': text_vectorization.get_config(),
        'weights': text_vectorization.get_weights(),
        'vocabulary': text_vectorization.get_vocabulary()
    }
    
    with open("saved_models/text_vectorizer.pkl", "wb") as f:
        pickle.dump(vectorizer_data, f)
    
    print("‚úÖ Vectorizador guardado en saved_models/")

# ============================================================================
# FUNCIONES DE ENTRENAMIENTO
# ============================================================================

def train_model_1(train_dataset, text_vectorization):
    """Entrenar espec√≠ficamente el Modelo 1"""
    print(f"\nüèãÔ∏è ENTRENANDO MODELO 1 ({Config.MODEL1_EPOCHS} √©pocas)")
    print("=" * 50)
    
    # Crear carpeta si no existe
    os.makedirs("saved_models", exist_ok=True)
    
    # Crear modelo
    model = create_model_1()
    
    # Callbacks para entrenamiento inteligente
    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor='loss', 
            patience=5,  # Parar si no mejora en 5 √©pocas
            restore_best_weights=True
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='loss',
            factor=0.5,
            patience=3
        )
    ]
    
    # Entrenar
    history = model.fit(
        train_dataset,
        epochs=Config.MODEL1_EPOCHS,
        callbacks=callbacks,
        verbose=1
    )
    
    # Guardar en la carpeta saved_models
    model.save("saved_models/movie_model_1.keras")
    save_vectorizer(text_vectorization)  # Guardar vectorizador tambi√©n
    
    final_loss = min(history.history['loss'])
    print(f"‚úÖ Modelo 1 entrenado - Mejor loss: {final_loss:.4f}")
    print("üíæ Guardado como: saved_models/movie_model_1.keras")

def train_model_2(train_dataset, text_vectorization):
    """Entrenar espec√≠ficamente el Modelo 2"""
    print(f"\nüèãÔ∏è ENTRENANDO MODELO 2 ({Config.MODEL2_EPOCHS} √©pocas)")
    print("=" * 50)
    
    # Crear carpeta si no existe
    os.makedirs("saved_models", exist_ok=True)
    
    # Crear modelo
    model = create_model_2()
    
    # Callbacks M√ÅS RESTRICTIVOS para evitar overfitting
    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor='loss', 
            patience=3,  # MENOS paciencia
            restore_best_weights=True
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='loss',
            factor=0.7,  # Reducci√≥n m√°s suave
            patience=2
        )
    ]
    
    # Entrenar
    history = model.fit(
        train_dataset,
        epochs=Config.MODEL2_EPOCHS,  # MENOS √©pocas
        callbacks=callbacks,
        verbose=1
    )
    
    # Guardar en la carpeta saved_models
    model.save("saved_models/movie_model_2.keras")
    save_vectorizer(text_vectorization)
    
    final_loss = min(history.history['loss'])
    print(f"‚úÖ Modelo 2 entrenado - Mejor loss: {final_loss:.4f}")
    print("üíæ Guardado como: saved_models/movie_model_2.keras")

# ============================================================================
# FUNCI√ìN PRINCIPAL
# ============================================================================

def main():
    """Funci√≥n principal con argumentos de l√≠nea de comandos"""
    parser = argparse.ArgumentParser(description='Entrenar modelos Transformer')
    parser.add_argument('--modelo', choices=['1', '2', 'all'], required=True,
                       help='Qu√© modelo entrenar: 1, 2, o all (ambos)')
    
    args = parser.parse_args()
    
    print("üöÄ ENTRENAMIENTO DE MODELOS TRANSFORMER")
    print("=" * 50)
    print(f"üéØ Entrenando: Modelo {args.modelo}")
    print(f"üìä Configuraci√≥n:")
    print(f"   - Vocabulario: {Config.VOCAB_SIZE} tokens")
    print(f"   - Secuencia: {Config.SEQUENCE_LENGTH} tokens")
    print(f"   - Muestras: {Config.SAMPLE_SIZE}")
    
    # Preparar datos (com√∫n para ambos)
    train_dataset, text_vectorization = prepare_data()
    
    # Entrenar seg√∫n selecci√≥n
    if args.modelo == '1':
        train_model_1(train_dataset, text_vectorization)
    elif args.modelo == '2':
        train_model_2(train_dataset, text_vectorization)
    elif args.modelo == 'all':
        train_model_1(train_dataset, text_vectorization)
        print("\n" + "="*30 + " MODELO 2 " + "="*30)
        train_model_2(train_dataset, text_vectorization)
    
    print("\nüéâ ¬°ENTRENAMIENTO COMPLETADO!")
    print("üöÄ Ahora puedes usar: streamlit run app.py")

# ============================================================================
# MODO INTERACTIVO (si no se usan argumentos)
# ============================================================================

def interactive_mode():
    """Modo interactivo si se ejecuta sin argumentos"""
    print("üé¨ ENTRENADOR DE MODELOS DE RESE√ëAS")
    print("=" * 40)
    print("¬øQu√© modelo quieres entrenar?")
    print("1. Modelo 1 (Simple, estable)")
    print("2. Modelo 2 (Doble, m√°s complejo)")
    print("3. Ambos modelos")
    print("4. Salir")
    
    while True:
        choice = input("\nElige una opci√≥n (1-4): ").strip()
        
        if choice in ['1', '2', '3']:
            # Preparar datos
            train_dataset, text_vectorization = prepare_data()
            
            if choice == '1':
                train_model_1(train_dataset, text_vectorization)
            elif choice == '2':
                train_model_2(train_dataset, text_vectorization)
            elif choice == '3':
                train_model_1(train_dataset, text_vectorization)
                train_model_2(train_dataset, text_vectorization)
            
            print("\n‚úÖ ¬°Entrenamiento completado!")
            break
            
        elif choice == '4':
            print("üëã ¬°Hasta luego!")
            break
        else:
            print("‚ùå Opci√≥n no v√°lida.")

if __name__ == "__main__":
    import sys
    
    # Si no hay argumentos, usar modo interactivo
    if len(sys.argv) == 1:
        interactive_mode()
    else:
        main()